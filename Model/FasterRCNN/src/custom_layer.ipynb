{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new layer for customized torch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/breast-tumor-detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import MyModel, create_model, MotionBlur\n",
    "import torch\n",
    "from dataset import train_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from config import DEVICE, NUM_CLASSES, NUM_EPOCHS, OUT_DIR\n",
    "from config import VISUALIZE_TRANSFORMED_IMAGES\n",
    "from config import SAVE_PLOTS_EPOCH, SAVE_MODEL_EPOCH\n",
    "from utils import Averager, show_tranformed_image\n",
    "from tqdm.auto import tqdm\n",
    "from dataset import train_loader, valid_loader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionBlur(torch.nn.Module):\n",
    "    \"\"\"Custom layer that picks one out of 8 motion blurs based\n",
    "    on gumble softmax probability distribution\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # The greater the size, the more the motion.\n",
    "        self.kernel_size = kernel_size\n",
    "        # Create the vertical kernel.\n",
    "        self.kernel_north = (\n",
    "            torch.zeros((1, 1,self.kernel_size, self.kernel_size))\n",
    "            .type(torch.FloatTensor)\n",
    "        ).to(DEVICE)\n",
    "        self.kernel_south = (\n",
    "            torch.zeros((1, 1,self.kernel_size, self.kernel_size))\n",
    "            .type(torch.FloatTensor)\n",
    "        ).to(DEVICE)\n",
    "        self.kernel_east = (\n",
    "            torch.zeros((1, 1,self.kernel_size, self.kernel_size))\n",
    "            .type(torch.FloatTensor)\n",
    "        ).to(DEVICE)\n",
    "        self.kernel_west = (\n",
    "            torch.zeros((1, 1,self.kernel_size, self.kernel_size))\n",
    "            .type(torch.FloatTensor)\n",
    "        ).to(DEVICE)\n",
    "        # Fill the middle row with ones.\n",
    "        self.kernel_north[0, 0,\n",
    "            : int((self.kernel_size) / 2), int((self.kernel_size - 1) / 2)\n",
    "        ] = torch.ones(int(self.kernel_size / 2))\n",
    "        self.kernel_south[0, 0,\n",
    "            int((self.kernel_size) / 2) :, int((self.kernel_size - 1) / 2)\n",
    "        ] = torch.ones(int(self.kernel_size / 2))\n",
    "        self.kernel_east[0, 0,\n",
    "            int((self.kernel_size - 1) / 2), int((self.kernel_size) / 2) :\n",
    "        ] = torch.ones(int(self.kernel_size / 2))\n",
    "        self.kernel_west[0, 0,\n",
    "            int((self.kernel_size - 1) / 2), : int((self.kernel_size) / 2)\n",
    "        ] = torch.ones(int(self.kernel_size / 2))\n",
    "\n",
    "        # Initialize the logits\n",
    "        self.logits = torch.nn.Parameter(torch.randn(4, device = DEVICE), requires_grad=True)\n",
    "        self.probs = F.gumbel_softmax(self.logits, tau=1, hard=False).to(DEVICE)\n",
    "\n",
    "        # Initialize motion convlutions and final image\n",
    "        self.final_image = torch.nn.Parameter(torch.zeros(1, 512, 512)).to(DEVICE)\n",
    "        self.motion_east = torch.zeros(1, 512, 512).to(DEVICE)\n",
    "        self.motion_west = torch.zeros(1, 512, 512).to(DEVICE)\n",
    "        self.motion_south = torch.zeros(1, 512, 512).to(DEVICE)\n",
    "        self.motion_north = torch.zeros(1, 512, 512).to(DEVICE)\n",
    "        self.stack = torch.zeros(4,1,512,512).to(DEVICE)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute convolutions\n",
    "        self.motion_east = F.conv2d(\n",
    "            x, self.kernel_east, stride=(1), padding=\"same\"\n",
    "        )\n",
    "        self.motion_west = F.conv2d(\n",
    "            x, self.kernel_west, stride=(1), padding=\"same\"\n",
    "        )\n",
    "        self.motion_south = F.conv2d(\n",
    "            x, self.kernel_south, stride=(1), padding=\"same\"\n",
    "        )\n",
    "        self.motion_north = F.conv2d(\n",
    "            x, self.kernel_north, stride=(1), padding=\"same\"\n",
    "        )\n",
    "        self.stack = torch.stack(\n",
    "            (self.motion_north, self.motion_south, self.motion_east, self.motion_west), dim=0\n",
    "        )\n",
    "        # Compute final image from gumble softmax probs\n",
    "        for idx, prob in enumerate(self.probs):\n",
    "            for blur in self.stack:\n",
    "                self.final_image = torch.add(self.final_image, torch.mul(prob, blur))\n",
    "        return self.final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import train_loader\n",
    "images, target = next(iter(train_loader))\n",
    "images = images[0].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [-7.0931e-17, -1.1256e-16, -1.0023e-16,  ...,  4.1633e-17,\n",
       "           2.9298e-17,  6.0137e-17],\n",
       "         [-7.0931e-17, -1.1256e-16, -1.0023e-16,  ...,  4.1633e-17,\n",
       "           2.9298e-17,  6.0137e-17],\n",
       "         [-7.0931e-17, -1.1256e-16, -1.0023e-16,  ...,  4.1633e-17,\n",
       "           2.9298e-17,  6.0137e-17]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motionblur = MotionBlur(30)\n",
    "motionblur(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "def create_model(num_classes):\n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    )\n",
    "    # get the number of input features\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    \"\"\"Define new Faster-RCNN module\"\"\"\n",
    "    def __init__(self, pretrained) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        self.motionblur = MotionBlur(30)\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        new_x = []\n",
    "        for item in x:\n",
    "            new_x.append(self.motionblur(item))\n",
    "        new_x = tuple(new_x)\n",
    "        return self.pretrained(new_x, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and move to the computation device\n",
    "model = MyModel(pretrained = create_model(NUM_CLASSES))\n",
    "model = model.cuda()\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# initialize the weight tracker class\n",
    "logit_weights = []\n",
    "\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('motionblur.logits', Parameter containing:\n",
      "tensor([ 0.4563, -0.4191,  0.0025, -1.2973], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "print(next(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motionblur.logits\n",
      "pretrained.backbone.body.layer2.0.conv1.weight\n",
      "pretrained.backbone.body.layer2.0.conv2.weight\n",
      "pretrained.backbone.body.layer2.0.conv3.weight\n",
      "pretrained.backbone.body.layer2.0.downsample.0.weight\n",
      "pretrained.backbone.body.layer2.1.conv1.weight\n",
      "pretrained.backbone.body.layer2.1.conv2.weight\n",
      "pretrained.backbone.body.layer2.1.conv3.weight\n",
      "pretrained.backbone.body.layer2.2.conv1.weight\n",
      "pretrained.backbone.body.layer2.2.conv2.weight\n",
      "pretrained.backbone.body.layer2.2.conv3.weight\n",
      "pretrained.backbone.body.layer2.3.conv1.weight\n",
      "pretrained.backbone.body.layer2.3.conv2.weight\n",
      "pretrained.backbone.body.layer2.3.conv3.weight\n",
      "pretrained.backbone.body.layer3.0.conv1.weight\n",
      "pretrained.backbone.body.layer3.0.conv2.weight\n",
      "pretrained.backbone.body.layer3.0.conv3.weight\n",
      "pretrained.backbone.body.layer3.0.downsample.0.weight\n",
      "pretrained.backbone.body.layer3.1.conv1.weight\n",
      "pretrained.backbone.body.layer3.1.conv2.weight\n",
      "pretrained.backbone.body.layer3.1.conv3.weight\n",
      "pretrained.backbone.body.layer3.2.conv1.weight\n",
      "pretrained.backbone.body.layer3.2.conv2.weight\n",
      "pretrained.backbone.body.layer3.2.conv3.weight\n",
      "pretrained.backbone.body.layer3.3.conv1.weight\n",
      "pretrained.backbone.body.layer3.3.conv2.weight\n",
      "pretrained.backbone.body.layer3.3.conv3.weight\n",
      "pretrained.backbone.body.layer3.4.conv1.weight\n",
      "pretrained.backbone.body.layer3.4.conv2.weight\n",
      "pretrained.backbone.body.layer3.4.conv3.weight\n",
      "pretrained.backbone.body.layer3.5.conv1.weight\n",
      "pretrained.backbone.body.layer3.5.conv2.weight\n",
      "pretrained.backbone.body.layer3.5.conv3.weight\n",
      "pretrained.backbone.body.layer4.0.conv1.weight\n",
      "pretrained.backbone.body.layer4.0.conv2.weight\n",
      "pretrained.backbone.body.layer4.0.conv3.weight\n",
      "pretrained.backbone.body.layer4.0.downsample.0.weight\n",
      "pretrained.backbone.body.layer4.1.conv1.weight\n",
      "pretrained.backbone.body.layer4.1.conv2.weight\n",
      "pretrained.backbone.body.layer4.1.conv3.weight\n",
      "pretrained.backbone.body.layer4.2.conv1.weight\n",
      "pretrained.backbone.body.layer4.2.conv2.weight\n",
      "pretrained.backbone.body.layer4.2.conv3.weight\n",
      "pretrained.backbone.fpn.inner_blocks.0.0.weight\n",
      "pretrained.backbone.fpn.inner_blocks.0.0.bias\n",
      "pretrained.backbone.fpn.inner_blocks.1.0.weight\n",
      "pretrained.backbone.fpn.inner_blocks.1.0.bias\n",
      "pretrained.backbone.fpn.inner_blocks.2.0.weight\n",
      "pretrained.backbone.fpn.inner_blocks.2.0.bias\n",
      "pretrained.backbone.fpn.inner_blocks.3.0.weight\n",
      "pretrained.backbone.fpn.inner_blocks.3.0.bias\n",
      "pretrained.backbone.fpn.layer_blocks.0.0.weight\n",
      "pretrained.backbone.fpn.layer_blocks.0.0.bias\n",
      "pretrained.backbone.fpn.layer_blocks.1.0.weight\n",
      "pretrained.backbone.fpn.layer_blocks.1.0.bias\n",
      "pretrained.backbone.fpn.layer_blocks.2.0.weight\n",
      "pretrained.backbone.fpn.layer_blocks.2.0.bias\n",
      "pretrained.backbone.fpn.layer_blocks.3.0.weight\n",
      "pretrained.backbone.fpn.layer_blocks.3.0.bias\n",
      "pretrained.rpn.head.conv.0.0.weight\n",
      "pretrained.rpn.head.conv.0.0.bias\n",
      "pretrained.rpn.head.cls_logits.weight\n",
      "pretrained.rpn.head.cls_logits.bias\n",
      "pretrained.rpn.head.bbox_pred.weight\n",
      "pretrained.rpn.head.bbox_pred.bias\n",
      "pretrained.roi_heads.box_head.fc6.weight\n",
      "pretrained.roi_heads.box_head.fc6.bias\n",
      "pretrained.roi_heads.box_head.fc7.weight\n",
      "pretrained.roi_heads.box_head.fc7.bias\n",
      "pretrained.roi_heads.box_predictor.cls_score.weight\n",
      "pretrained.roi_heads.box_predictor.cls_score.bias\n",
      "pretrained.roi_heads.box_predictor.bbox_pred.weight\n",
      "pretrained.roi_heads.box_predictor.bbox_pred.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2310423 ,  0.90460676,  0.25764972, -0.8289195 ], dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "prog_bar = tqdm(train_loader, total=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for data in prog_bar:\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        images = [image.to(DEVICE) for image in images]\n",
    "        targets = [\n",
    "            {k: v.to(DEVICE, dtype=torch.int64) for k, v in t.items()} for t in targets\n",
    "        ]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        \n",
    "        losses.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_itr += 1\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([nan, nan, nan, nan], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
